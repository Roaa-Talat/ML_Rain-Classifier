# Machine Learning Rain Prediction Model

---

## ğŸ“– Project Overview
This project focuses on analyzing a weather dataset to predict the likelihood of rain using machine learning models. The dataset contains key weather features and a binary target variable (`rain` or `no rain`).

### ğŸ¯ Objectives:
- Load and explore the dataset.
- Handle missing values using different techniques.
- Analyze data distributions and feature scaling.
- Train and evaluate multiple classification models:
  - **Decision Tree**
  - **k-Nearest Neighbors (kNN)** (both built-in and custom implementation)
  - **NaÃ¯ve Bayes**
- Compare model performance using evaluation metrics.

---

## ğŸ“Š Dataset Overview
### ğŸ” 1. Dataset Details
âœ” 2,500 rows, 6 columns covering various weather conditions.
âœ” **Features:** Temperature, Humidity, Wind Speed, Cloud Cover, Pressure.
âœ” **Target Variable:** Rain (binary classification: 0 = no rain, 1 = rain).

### ğŸ” 2. Missing Values Analysis
- The dataset contains missing values:
  - **Temperature:** 25 missing
  - **Humidity:** 40 missing
  - **Wind Speed:** 32 missing
  - **Cloud Cover:** 33 missing
  - **Pressure:** 27 missing
  - **Rain:** No missing values

---

## ğŸ›  Data Preprocessing
### âœ… Steps:

1ï¸âƒ£ **Handling Missing Values**
   - Method 1: Replace missing values with the feature mean.
   - Method 2: Drop rows containing missing values.

2ï¸âƒ£ **Feature Scaling**
   - Applied **Standardization (Z-score normalization)** after splitting data to avoid data leakage.
   
3ï¸âƒ£ **Data Splitting**
   - **80% Training** / **20% Testing** using `train_test_split` from Scikit-learn.
   
4ï¸âƒ£ **Encoding Target Variable**
   - Converted categorical values (`rain`/`no rain`) into binary numeric values (0 or 1).

---

## ğŸ¤– Machine Learning Models
### ğŸ“Œ 1. **Decision Tree Classifier**
âœ” Optimized splitting using **Entropy and Information Gain**.
âœ” Evaluated on different missing value handling techniques.
âœ” Model correctly classified **99% of 'No Rain' cases**, but struggled with **'Rain' cases**.

### ğŸ“Œ 2. **k-Nearest Neighbors (kNN)**
âœ” Implemented both **built-in Scikit-learn kNN** and **custom kNN from scratch**.
âœ” kNN performed well but suffered from false negatives for predicting rain.
âœ” Best performance was achieved with **k = 11**.

### ğŸ“Œ 3. **NaÃ¯ve Bayes Classifier**
âœ” Handled binary classification efficiently.
âœ” Accuracy remained **96%**, but recall for `Rain` was lower.
âœ” Model struggled with **class imbalance**, misclassifying some rain instances.

---

## ğŸ“Š Model Performance Comparison
| Metric         | Decision Tree | kNN (k=11) | NaÃ¯ve Bayes |
|---------------|--------------|------------|-------------|
| **Accuracy**  | 100%         | 97%        | 96%         |
| **Precision (Rain)** | 1.00 | 0.92 | 1.00 |
| **Recall (Rain)** | 0.96 | 0.79 | 0.68 |
| **F1-Score (Rain)** | 0.98 | 0.85 | 0.81 |
| **False Positives** | 0 | 4 | 0 |
| **False Negatives** | 2 | 12 | 18 |

---

## ğŸš€ Key Takeaways
âœ… Decision Tree provided the highest accuracy but overfitted slightly.

âœ… kNN performed well, but optimal **k-value selection is crucial**.

âœ… NaÃ¯ve Bayes worked well but suffered from class imbalance.

âœ… Handling missing data impacts final model performance significantly.

---

## ğŸ›  Technologies & Libraries Used
ğŸ”¹ **Python**

ğŸ”¹ **Pandas, NumPy** (Data manipulation)

ğŸ”¹ **Matplotlib, Seaborn** (Data visualization)

ğŸ”¹ **Scikit-learn** (ML models & evaluation)

---

## ğŸ”® Future Enhancements
ğŸ”¹ Implement **Random Forest** to improve decision tree generalization.

ğŸ”¹ Balance the dataset using **oversampling techniques**.

ğŸ”¹ Experiment with **deep learning models** for improved accuracy.

